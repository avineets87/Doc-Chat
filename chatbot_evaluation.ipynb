{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/avineetsharma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Ensure you have the necessary nltk data\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is attention?\n",
      "Expected: The capital of France is Paris.\n",
      "Generated: Attention is a function that maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum. It is a mechanism that relates different positions of a single sequence to compute a representation of the sequence. This is often referred to as self-attention or intra-attention. \n",
      "\n",
      "The provided knowledge highlights the potential of attention-based models, particularly in creating more interpretable models. By inspecting attention distributions, one can gain insights into the model's inner workings and make it more understandable. The text also mentions the Transformer, a model that utilizes attention, which the authors plan to extend to various input and output modalities beyond text.\n",
      "BLEU Score: 0.0021\n",
      "\n",
      "Query: What all is taught in AI Course?\n",
      "Expected: William Shakespeare wrote Hamlet.\n",
      "Generated: The AI Course, as described in the provided knowledge, covers a wide range of topics related to artificial intelligence and machine learning. Here's an overview of what is taught:\n",
      "\n",
      "**Section 1: Programming for AI**\n",
      "- Introduction to AI and machine learning fundamentals.\n",
      "- Python fundamentals, focusing on data manipulation for training AI models.\n",
      "- Making predictions using data.\n",
      "\n",
      "**Section 2: Machine Learning**\n",
      "- Supervised and Unsupervised Machine Learning techniques.\n",
      "- Natural Language Processing (NLP).\n",
      "- Time Series Forecasting.\n",
      "\n",
      "**Section 3: AI Innovations**\n",
      "- Advanced AI and machine learning topics.\n",
      "- Complex neural networks and deep learning.\n",
      "- AI ethics and data regulations to ensure legal and ethical usage of AI skills.\n",
      "\n",
      "The course seems to be designed to provide a comprehensive understanding of AI, starting from the basics of programming and Python to more advanced concepts in machine learning and AI innovations. It also emphasizes the practical application of these skills through various modules and group projects.\n",
      "BLEU Score: 0.0009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample test cases with queries, expected responses, and RAG-generated responses\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What is attention?\",\n",
    "        \"expected\": \"The capital of France is Paris.\",\n",
    "        \"generated\": \"Attention is a function that maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum. It is a mechanism that relates different positions of a single sequence to compute a representation of the sequence. This is often referred to as self-attention or intra-attention. \\n\\nThe provided knowledge highlights the potential of attention-based models, particularly in creating more interpretable models. By inspecting attention distributions, one can gain insights into the model's inner workings and make it more understandable. The text also mentions the Transformer, a model that utilizes attention, which the authors plan to extend to various input and output modalities beyond text.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What all is taught in AI Course?\",\n",
    "        \"expected\": \"William Shakespeare wrote Hamlet.\",\n",
    "        \"generated\": \"The AI Course, as described in the provided knowledge, covers a wide range of topics related to artificial intelligence and machine learning. Here's an overview of what is taught:\\n\\n**Section 1: Programming for AI**\\n- Introduction to AI and machine learning fundamentals.\\n- Python fundamentals, focusing on data manipulation for training AI models.\\n- Making predictions using data.\\n\\n**Section 2: Machine Learning**\\n- Supervised and Unsupervised Machine Learning techniques.\\n- Natural Language Processing (NLP).\\n- Time Series Forecasting.\\n\\n**Section 3: AI Innovations**\\n- Advanced AI and machine learning topics.\\n- Complex neural networks and deep learning.\\n- AI ethics and data regulations to ensure legal and ethical usage of AI skills.\\n\\nThe course seems to be designed to provide a comprehensive understanding of AI, starting from the basics of programming and Python to more advanced concepts in machine learning and AI innovations. It also emphasizes the practical application of these skills through various modules and group projects.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Compute the BLEU score between reference and hypothesis texts.\n",
    "    Uses smoothing to handle short sentences.\n",
    "    \"\"\"\n",
    "    reference_tokens = [nltk.word_tokenize(reference.lower())]  # Convert to list of lists\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "\n",
    "    # Smoothing function to avoid zero scores for short responses\n",
    "    smoothie = SmoothingFunction().method1\n",
    "\n",
    "    bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=smoothie)\n",
    "    return round(bleu_score, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate BLEU scores for test cases\n",
    "for case in test_cases:\n",
    "    score = calculate_bleu(case[\"expected\"], case[\"generated\"])\n",
    "    print(f\"Query: {case['query']}\")\n",
    "    print(f\"Expected: {case['expected']}\")\n",
    "    print(f\"Generated: {case['generated']}\")\n",
    "    print(f\"BLEU Score: {score}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
