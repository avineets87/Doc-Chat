{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/avineetsharma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Ensure you have the necessary nltk data\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'What is attention?',\n",
       "  'expected': 'The capital of France is Paris.',\n",
       "  'generated': 'In the context of this knowledge, attention refers to a mechanism in a machine learning model, specifically the Transformer model. It involves mapping a query and a set of key-value pairs to an output using vectors. This process computes a weighted sum, allowing the model to relate different positions of a single sequence and generate a representation of that sequence.\\n\\nSelf-attention, or intra-attention, is a type of attention mechanism that has been successfully applied in various tasks. It enables the model to focus on specific parts of the input sequence, making the model more interpretable. The knowledge provided includes an example of visualizing attention distributions from different attention heads, which can offer insights into how the model processes information.\\n\\nThe authors express excitement about the future of attention-based models and their potential applications in various tasks and modalities beyond text.'},\n",
       " {'query': 'What all is taught in AI Course?',\n",
       "  'expected': 'William Shakespeare wrote Hamlet.',\n",
       "  'generated': 'The AI course covers a wide range of topics related to Artificial Intelligence and Machine Learning. Here is an overview of what is taught in the course:\\n\\n**Section 1: Programming for AI**\\n- Introduction to AI and its impact.\\n- Python fundamentals: This includes learning the skills necessary to manipulate data for training AI models.\\n- Making predictions using data.\\n\\n**Section 2: Machine Learning**\\n- Supervised and Unsupervised Machine Learning techniques.\\n- Natural Language Processing (NLP).\\n- Time Series Forecasting.\\n- AI Applications.\\n\\n**Section 3: AI Innovations**\\n- AI ethics and data regulations: Ensuring legal and ethical usage of AI skills.\\n- Advanced AI and Machine Learning topics: Covering complex neural networks and deep learning.\\n- Transformers.\\n\\nThe course aims to provide practical and technical skills to solve data, machine learning, and artificial intelligence problems. It also focuses on marketable technologies and skills, ensuring students gain proficiency in Python, Machine Learning algorithms, and various AI applications.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('test_cases.json', 'r') as file:\n",
    "    test_cases = json.load(file)\n",
    "\n",
    "test_cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Compute the BLEU score between reference and hypothesis texts.\n",
    "    Uses smoothing to handle short sentences.\n",
    "    \"\"\"\n",
    "    reference_tokens = [nltk.word_tokenize(reference.lower())]  # Convert to list of lists\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "\n",
    "    # Smoothing function to avoid zero scores for short responses\n",
    "    smoothie = SmoothingFunction().method1\n",
    "\n",
    "    bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=smoothie)\n",
    "    return round(bleu_score, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is attention?\n",
      "Expected: The capital of France is Paris.\n",
      "Generated: Attention is a function that maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum. It is a mechanism that relates different positions of a single sequence to compute a representation of the sequence. This is often referred to as self-attention or intra-attention. \n",
      "\n",
      "The provided knowledge highlights the potential of attention-based models, particularly in creating more interpretable models. By inspecting attention distributions, one can gain insights into the model's inner workings and make it more understandable. The text also mentions the Transformer, a model that utilizes attention, which the authors plan to extend to various input and output modalities beyond text.\n",
      "BLEU Score: 0.0021\n",
      "\n",
      "Query: What all is taught in AI Course?\n",
      "Expected: William Shakespeare wrote Hamlet.\n",
      "Generated: The AI Course, as described in the provided knowledge, covers a wide range of topics related to artificial intelligence and machine learning. Here's an overview of what is taught:\n",
      "\n",
      "**Section 1: Programming for AI**\n",
      "- Introduction to AI and machine learning fundamentals.\n",
      "- Python fundamentals, focusing on data manipulation for training AI models.\n",
      "- Making predictions using data.\n",
      "\n",
      "**Section 2: Machine Learning**\n",
      "- Supervised and Unsupervised Machine Learning techniques.\n",
      "- Natural Language Processing (NLP).\n",
      "- Time Series Forecasting.\n",
      "\n",
      "**Section 3: AI Innovations**\n",
      "- Advanced AI and machine learning topics.\n",
      "- Complex neural networks and deep learning.\n",
      "- AI ethics and data regulations to ensure legal and ethical usage of AI skills.\n",
      "\n",
      "The course seems to be designed to provide a comprehensive understanding of AI, starting from the basics of programming and Python to more advanced concepts in machine learning and AI innovations. It also emphasizes the practical application of these skills through various modules and group projects.\n",
      "BLEU Score: 0.0009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate BLEU scores for test cases\n",
    "for case in test_cases:\n",
    "    score = calculate_bleu(case[\"expected\"], case[\"generated\"])\n",
    "    print(f\"Query: {case['query']}\")\n",
    "    print(f\"Expected: {case['expected']}\")\n",
    "    print(f\"Generated: {case['generated']}\")\n",
    "    print(f\"BLEU Score: {score}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
